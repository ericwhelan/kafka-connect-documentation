===========================
SpoolDirJsonSourceConnector
===========================

.. image:: SpoolDirJsonSourceConnector.svg


This connector is used to `stream <https://en.wikipedia.org/wiki/JSON_Streaming>` JSON files from a directory while converting the data based on the schema supplied in the configuration.



-------------
Configuration
-------------

.. csv-table:: Configuration
    :header: "Name", "Type", "Importance", "Default Value", "Validator", "Documentation"
    :widths: auto

    "error.path","String","High","","com.github.jcustenborder.kafka.connect.utils.config.validators.filesystem.ValidDirectoryWritable@50588ef6","The directory to place files in which have error(s). This directory must exist and be writable by the user running Kafka Connect."
    "finished.path","String","High","","com.github.jcustenborder.kafka.connect.utils.config.validators.filesystem.ValidDirectoryWritable@10744fff","The directory to place files that have been successfully processed. This directory must exist and be writable by the user running Kafka Connect."
    "input.file.pattern","String","High","","","Regular expression to check input file names against. This expression must match the entire filename. The equivalent of Matcher.matches()."
    "input.path","String","High","","com.github.jcustenborder.kafka.connect.utils.config.validators.filesystem.ValidDirectoryWritable@2662997a","The directory to read files that will be processed. This directory must exist and be writable by the user running Kafka Connect."
    "topic","String","High","","","The Kafka topic to write the data to."
    "halt.on.error","Boolean","High","true","","Should the task halt when it encounters an error or continue to the next file."
    "key.schema","String","High","","","The schema for the key written to Kafka."
    "value.schema","String","High","","","The schema for the value written to Kafka."
    "schema.generation.enabled","Boolean","Medium","false","","Flag to determine if schemas should be dynamically generated. If set  to true, `key.schema` and `value.schema` can be omitted, but `schema.generation.key.name` and `schema.generation.value.name` must be set."
    "schema.generation.key.fields","List","Medium","[]","","The field(s) to use to build a key schema. This is only used during schema generation."
    "schema.generation.key.name","String","Medium","com.github.jcustenborder.kafka.connect.model.Key","","The name of the generated key schema."
    "schema.generation.value.name","String","Medium","com.github.jcustenborder.kafka.connect.model.Value","","The name of the generated value schema."
    "timestamp.field","String","Medium","","","The field in the value schema that will contain the parsed timestamp for the record. This field cannot be marked as optional and must be a [Timestamp](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/data/Schema.html)"
    "timestamp.mode","String","Medium","PROCESS_TIME","ValidEnum{enum=TimestampMode, allowed=[FIELD, FILE_TIME, PROCESS_TIME]}","Determines how the connector will set the timestamp for the [ConnectRecord](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/connector/ConnectRecord.html#timestamp()). If set to `Field` then the timestamp will be read from a field in the value. This field cannot be optional and must be a [Timestamp](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/data/Schema.html). Specify the field  in `timestamp.field`. If set to `FILE_TIME` then the last modified time of the file will be used. If set to `PROCESS_TIME` the time the record is read will be used."
    "batch.size","Int","Low","1000","","The number of records that should be returned with each batch."
    "empty.poll.wait.ms","Long","Low","1000","[1,...,9223372036854775807]","The amount of time to wait if a poll returns an empty list of records."
    "file.minimum.age.ms","Long","Low","0","[0,...,9223372036854775807]","The amount of time in milliseconds after the file was last written to before the file can be processed."
    "parser.timestamp.date.formats","List","Low","[yyyy-MM-dd'T'HH:mm:ss, yyyy-MM-dd' 'HH:mm:ss]","","The date formats that are expected in the file. This is a list of strings that will be used to parse the date fields in order. The most accurate date format should be the first in the list. Take a look at the Java documentation for more info. https://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html"
    "parser.timestamp.timezone","String","Low","UTC","","The timezone that all of the dates will be parsed with."
    "processing.file.extension","String","Low",".PROCESSING","ValidPattern{pattern=^.*\..+$}","Before a file is processed, it is renamed to indicate that it is currently being processed. This setting is appended to the end of the file."


^^^^^^^^^^^^^^^^^^^^^^
Property based example
^^^^^^^^^^^^^^^^^^^^^^


This configuration is used typically along with `standalone mode
<http://docs.confluent.io/current/connect/concepts.html#standalone-workers>`_.

.. code-block:: properties

    name=connector1
    tasks.max=1
    connector.class=com.github.jcustenborder.kafka.connect.spooldir.SpoolDirJsonSourceConnector
    # The following values must be configured.
    error.path=
    finished.path=
    input.file.pattern=
    input.path=
    topic=
    halt.on.error=
    key.schema=
    value.schema=
    schema.generation.enabled=
    schema.generation.key.fields=
    schema.generation.key.name=
    schema.generation.value.name=
    timestamp.field=
    timestamp.mode=
    batch.size=
    empty.poll.wait.ms=
    file.minimum.age.ms=
    parser.timestamp.date.formats=
    parser.timestamp.timezone=
    processing.file.extension=



^^^^^^^^^^^^^^^^^^
Rest based example
^^^^^^^^^^^^^^^^^^


This configuration is used typically along with `distributed mode
<http://docs.confluent.io/current/connect/concepts.html#distributed-workers>`_.
Write the following json to `connector.json`, configure all of the required values, and use the command below to
post the configuration to one the distributed connect worker(s).

.. code-block:: json

    {
        "name": "connector1",
        "config": {
            "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirJsonSourceConnector",
            "error.path":"",
            "finished.path":"",
            "input.file.pattern":"",
            "input.path":"",
            "topic":"",
            "halt.on.error":"",
            "key.schema":"",
            "value.schema":"",
            "schema.generation.enabled":"",
            "schema.generation.key.fields":"",
            "schema.generation.key.name":"",
            "schema.generation.value.name":"",
            "timestamp.field":"",
            "timestamp.mode":"",
            "batch.size":"",
            "empty.poll.wait.ms":"",
            "file.minimum.age.ms":"",
            "parser.timestamp.date.formats":"",
            "parser.timestamp.timezone":"",
            "processing.file.extension":"",
        }
    }

Use curl to post the configuration to one of the Kafka Connect Workers. Change `http://localhost:8083/` the the endpoint of
one of your Kafka Connect worker(s).

.. code-block:: bash

    curl -s -X POST -H 'Content-Type: application/json' --data @connector.json http://localhost:8083/connectors



